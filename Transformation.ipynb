{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61ec49e8",
   "metadata": {},
   "source": [
    "# 3. Transformation\n",
    "We will  address splitting the data, but also the main goals of Transformation, reshaping the data to fir our algorithms. Specifically we should focus correct data types and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45f3627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\komit\\Documents\\Education\\University\\University of Mannheim\\1st Sem\\Data Mining (IE500)\\3. Group Project\\datamining_group12\n"
     ]
    }
   ],
   "source": [
    "import sys, site, platform, pandas as pd\n",
    "import os\n",
    "from utils_io import load_step, save_step\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "print(os.getcwd())\n",
    "df = load_step(\"step2_preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438f2bf",
   "metadata": {},
   "source": [
    "### Boolean Values\n",
    "Since \"explicit\" is boolean, we need to convert it to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c807ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Explicit Attribute from boolean to int\n",
    "df[\"explicit\"] = df[\"explicit\"].astype(int)\n",
    "\n",
    "# # Verify that we have converted this correctly\n",
    "# print('Dataset Info without Irrelevant Features:')\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d43f82",
   "metadata": {},
   "source": [
    "### Object Attributes\n",
    "We are now ready to get rid of the \"track_id\" and \"track_name\" features. We can also drop the features which we converted using One-Hot Encoding such as \"key\" and \"time_signature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4c40083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing track_id, track_name and key, time_siganture\n",
    "df = df.drop(columns=['track_id', 'track_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431a121",
   "metadata": {},
   "source": [
    "### Splitting\n",
    "We are splitting our Target and our Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac2fe144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Target Attribute\n",
    "target = \"popularity\"\n",
    "\n",
    "# Spliting Target and Features used for prediction\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568e63c",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "Some of our numerical values are way larger than others. We should solve this issue by scaling our data. As mentioned, we will use StandardScaler. It is important not to do this AFTER splitting our data so as to avoid data leakage. We choose to split the \"duration_ms\", \"tempo\" and \"loudness\" features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94c190fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Choosing Numeric Columns we want to Scale\n",
    "cols_to_scale = ['duration_ms', 'tempo', 'loudness']\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Scale \n",
    "X_train_scaled[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
    "X_test_scaled[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab71a2",
   "metadata": {},
   "source": [
    "## Save Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "066884fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved step3_X_train.csv\n",
      "Saved step3_X_test.csv\n",
      "Saved step3_y_train.csv\n",
      "Saved step3_y_test.csv\n",
      "Saved step3_X_train_scaled.csv\n",
      "Saved step3_X_test_scaled.csv\n"
     ]
    }
   ],
   "source": [
    "save_step(X_train, \"step3_X_train\")\n",
    "save_step(X_test, \"step3_X_test\")\n",
    "save_step(y_train, \"step3_y_train\")\n",
    "save_step(y_test, \"step3_y_test\")\n",
    "\n",
    "save_step(X_train_scaled, \"step3_X_train_scaled\")\n",
    "save_step(X_test_scaled, \"step3_X_test_scaled\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
