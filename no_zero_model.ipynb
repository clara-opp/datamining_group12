{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dfda91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_io import load_step, save_step\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "y_train = load_step(\"y_train_nz\")\n",
    "y_test = load_step(\"y_test_nz\")\n",
    "X_train_scaled = load_step(\"X_train_nz_scaled\")\n",
    "X_test_scaled = load_step(\"X_test_nz_scaled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf5eeb",
   "metadata": {},
   "source": [
    "Lasso Model Tree, no zeros same fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0329dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "\n",
    "class LassoModelTree(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Very simple model tree:\n",
    "      1. Train a regression tree to split the feature space.\n",
    "      2. In each leaf, fit a separate Lasso regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_depth=None,\n",
    "                 min_samples_leaf=20,\n",
    "                 tree_random_state=42,\n",
    "                 lasso_alpha=0.01,\n",
    "                 lasso_max_iter=10000):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree_random_state = tree_random_state\n",
    "        self.lasso_alpha = lasso_alpha\n",
    "        self.lasso_max_iter = lasso_max_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y).ravel()\n",
    "\n",
    "        # 1) Fit a regression tree for the splits\n",
    "        self.tree_ = DecisionTreeRegressor(\n",
    "            criterion=\"squared_error\",\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            random_state=self.tree_random_state,\n",
    "        )\n",
    "        self.tree_.fit(X, y)\n",
    "\n",
    "        # 2) For each leaf, fit a Lasso model on the samples in that leaf\n",
    "        leaf_ids = self.tree_.apply(X)\n",
    "        self.leaf_models_ = {}\n",
    "\n",
    "        for leaf in np.unique(leaf_ids):\n",
    "            mask = leaf_ids == leaf\n",
    "            X_leaf = X[mask]\n",
    "            y_leaf = y[mask]\n",
    "\n",
    "            model = Lasso(alpha=self.lasso_alpha, max_iter=self.lasso_max_iter)\n",
    "            model.fit(X_leaf, y_leaf)\n",
    "            self.leaf_models_[leaf] = model\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        leaf_ids = self.tree_.apply(X)\n",
    "        y_pred = np.empty(X.shape[0])\n",
    "\n",
    "        for leaf in np.unique(leaf_ids):\n",
    "            mask = leaf_ids == leaf\n",
    "            model = self.leaf_models_[leaf]\n",
    "            y_pred[mask] = model.predict(X[mask])\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6823e01a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of labels=63097 does not match number of samples=70612",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m y_test_arr = np.ravel(y_test)\n\u001b[32m      5\u001b[39m mt_lasso = LassoModelTree(\n\u001b[32m      6\u001b[39m     max_depth=\u001b[38;5;28;01mNone\u001b[39;00m,        \u001b[38;5;66;03m# tune as needed\u001b[39;00m\n\u001b[32m      7\u001b[39m     min_samples_leaf=\u001b[32m200\u001b[39m,\n\u001b[32m      8\u001b[39m     lasso_alpha=\u001b[32m0.1\u001b[39m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mmt_lasso\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m y_pred_mt_lasso = mt_lasso.predict(X_test_scaled)\n\u001b[32m     14\u001b[39m mse = mean_squared_error(y_test_arr, y_pred_mt_lasso)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mLassoModelTree.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 1) Fit a regression tree for the splits\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mself\u001b[39m.tree_ = DecisionTreeRegressor(\n\u001b[32m     31\u001b[39m     criterion=\u001b[33m\"\u001b[39m\u001b[33msquared_error\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     max_depth=\u001b[38;5;28mself\u001b[39m.max_depth,\n\u001b[32m     33\u001b[39m     min_samples_leaf=\u001b[38;5;28mself\u001b[39m.min_samples_leaf,\n\u001b[32m     34\u001b[39m     random_state=\u001b[38;5;28mself\u001b[39m.tree_random_state,\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 2) For each leaf, fit a Lasso model on the samples in that leaf\u001b[39;00m\n\u001b[32m     39\u001b[39m leaf_ids = \u001b[38;5;28mself\u001b[39m.tree_.apply(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/datamining_group12/venv/lib/python3.14/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/datamining_group12/venv/lib/python3.14/site-packages/sklearn/tree/_classes.py:1404\u001b[39m, in \u001b[36mDecisionTreeRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1376\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[32m   1377\u001b[39m \n\u001b[32m   1378\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/datamining_group12/venv/lib/python3.14/site-packages/sklearn/tree/_classes.py:355\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    352\u001b[39m max_leaf_nodes = -\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.max_leaf_nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.max_leaf_nodes\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y) != n_samples:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    356\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNumber of labels=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m does not match number of samples=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    357\u001b[39m         % (\u001b[38;5;28mlen\u001b[39m(y), n_samples)\n\u001b[32m    358\u001b[39m     )\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    361\u001b[39m     sample_weight = _check_sample_weight(sample_weight, X, dtype=DOUBLE)\n",
      "\u001b[31mValueError\u001b[39m: Number of labels=63097 does not match number of samples=70612"
     ]
    }
   ],
   "source": [
    "# ensure y is 1D\n",
    "y_train_arr = np.ravel(y_train)\n",
    "y_test_arr = np.ravel(y_test)\n",
    "\n",
    "mt_lasso = LassoModelTree(\n",
    "    max_depth=None,        # tune as needed\n",
    "    min_samples_leaf=200,\n",
    "    lasso_alpha=0.1,\n",
    ")\n",
    "\n",
    "mt_lasso.fit(X_train_scaled, y_train_arr)\n",
    "y_pred_mt_lasso = mt_lasso.predict(X_test_scaled)\n",
    "\n",
    "mse = mean_squared_error(y_test_arr, y_pred_mt_lasso)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_arr, y_pred_mt_lasso)\n",
    "\n",
    "print(\"Lasso Model Tree\")\n",
    "print(f\"MSE :  {mse:.4f}\")\n",
    "print(f\"RMSE:  {rmse:.4f}\")\n",
    "print(f\"R^2 :  {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
